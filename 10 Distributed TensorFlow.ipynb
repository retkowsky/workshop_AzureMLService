{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 10. Distributed TensorFlow with parameter server\nIn this tutorial, you will train a TensorFlow model on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset using native [distributed TensorFlow](https://www.tensorflow.org/deploy/distributed)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1. Prerequisites\n* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning (AML)\n* Go through the [00.configuration.ipynb](https://github.com/Azure/MachineLearningNotebooks/blob/master/00.configuration.ipynb) notebook to:\n    * install the AML SDK\n    * create a workspace and its configuration file (`config.json`)\n* Review the [tutorial](https://aka.ms/aml-notebook-hyperdrive) on single-node TensorFlow training using the SDK"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Check core SDK version number\nimport azureml.core\n\nprint(\"SDK version:\", azureml.core.VERSION)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SDK version: 1.0.2\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Diagnostics\nOpt-in diagnostics for better experience, quality, and security of future releases."
    },
    {
      "metadata": {
        "tags": [
          "Diagnostics"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.telemetry import set_diagnostics_collection\n\nset_diagnostics_collection(send_diagnostics=True)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Initialize workspace\nInitialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.workspace import Workspace\n\nws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Subscription id: ' + ws.subscription_id, \n      'Resource group: ' + ws.resource_group, sep = '\\n')",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/aml_config/config.json\nWorkspace name: AMLSworkspace\nAzure region: westeurope\nSubscription id: 70b8f39e-8863-49f7-b6ba-34a80799550c\nResource group: resgrpAMLS\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2. Create a remote compute target\nYou will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) to execute your training script on. In this tutorial, you create an `AmlCompute` cluster as your training compute resource. This code creates a cluster for you if it does not already exist in your workspace.\n\n**Creation of the cluster takes approximately 5 minutes.** If the cluster is already in your workspace this code will skip the cluster creation process."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\n\n# choose a name for your cluster\ncluster_name = \"gpucluster\"\n\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing compute target.')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n                                                           max_nodes=4)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)\n\n# Use the 'status' property to get a detailed status for the current cluster. \nprint(compute_target.status.serialize())",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating a new compute target...\nCreating\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n{'allocationState': 'Steady', 'allocationStateTransitionTime': '2018-12-10T16:11:13.061000+00:00', 'creationTime': '2018-12-10T16:09:58.703952+00:00', 'currentNodeCount': 0, 'errors': None, 'modifiedTime': '2018-12-10T16:11:24.446527+00:00', 'nodeStateCounts': {'idleNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0, 'preparingNodeCount': 0, 'runningNodeCount': 0, 'unusableNodeCount': 0}, 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'targetNodeCount': 0, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3. Train model on the remote compute\nNow that we have the cluster ready to go, let's run our distributed training job."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.1 Create a project directory\nCreate a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on3.1."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\nproject_folder = './tf-distr-ps'\nos.makedirs(project_folder, exist_ok=True)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copy the training script `tf_mnist_replica.py` into this project directory."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\n\nshutil.copy('tf_mnist_replica.py', project_folder)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "'./tf-distr-ps/tf_mnist_replica.py'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Create an experiment\nCreate an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace for this distributed TensorFlow tutorial. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Experiment\n\nexperiment_name = 'tf-distr-ps'\nexperiment = Experiment(ws, name=experiment_name)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Create a TensorFlow estimator\nThe AML SDK's TensorFlow estimator enables you to easily submit TensorFlow training jobs for both single-node and distributed runs. For more information on the TensorFlow estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-tensorflow)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.dnn import TensorFlow\n\nscript_params={\n    '--num_gpus': 1,\n    '--train_steps': 500\n}\n\nestimator = TensorFlow(source_directory=project_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='tf_mnist_replica.py',\n                       node_count=2,\n                       worker_count=2,\n                       parameter_server_count=1,   \n                       distributed_backend='ps',\n                       use_gpu=True)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The above code specifies that we will run our training script on `2` nodes, with two workers and one parameter server. In order to execute a native distributed TensorFlow run, you must provide the argument `distributed_backend='ps'`. Using this estimator with these settings, TensorFlow and its dependencies will be installed for you. However, if your script also uses other packages, make sure to install them via the `TensorFlow` constructor's `pip_packages` or `conda_packages` parameters."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.4 Submit job\nRun your experiment by submitting your estimator object. Note that this call is asynchronous."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = experiment.submit(estimator)\nprint(run)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Run(Experiment: tf-distr-ps,\nId: tf-distr-ps_1544458335655,\nType: azureml.scriptrun,\nStatus: Starting)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.5 Monitor your run\nYou can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\n\nRunDetails(run).show()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ff259bb98d143939138428d488df6e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Alternatively, you can block until the script has completed training before running more code."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# run.wait_for_completion(show_output=True) # this provides a verbose log",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "minxia"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "msauthor": "minxia"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}